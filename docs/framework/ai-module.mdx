---
title: "Writer AI module"
---

This module leverages the Writer SDK to enable applications to interact with large language models (LLMs) in chat or text completion formats. It provides tools to manage conversation states and to dynamically interact with LLMs using both synchronous and asynchronous methods.

## Conversation class
The `Conversation` class manages LLM communications within a chat framework, storing the conversation history and handling the interactions.

```python
import writer as wf
import writer.ai

def handle_simple_message(state, payload):
    # Update the conversation state by appending the incoming user message.
    state["conversation"] += payload
    
    # Stream the complete response from the AI model in chunks.
    for chunk in state["conversation"].stream_complete():
        # Append each chunk of the model's response to the ongoing conversation state.
        state["conversation"] += chunk

# Initialize the application state with a new Conversation object.
initial_state = wf.init_state({
    "conversation": writer.ai.Conversation(),
})
```

### Initializing a conversation
A `Conversation` can be initialized with either a system prompt or a list of previous messages. It can also accept a default configuration dictionary that sets parameters for all interactions.


```python
# Initialize with a system prompt
conversation = Conversation("You assist Chinese-speaking clients with their questions")

# Initialize with a history of messages
history = [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi, how can I help?"}
]
conversation = Conversation(history)

# Initialize with a configuration
config = {'max_tokens': 150, 'temperature': 0.7}
conversation = Conversation("You are a social media expert in the financial industry", config=config)
```

### Adding messages to conversation
Messages can be added to a `Conversation` instance using the `+` operator or the `add` method.

```python
# Using the `+` operator
conversation += {"role": "user", "content": "What's the weather like?"}

# Using the `add` method
conversation.add(role="user", content="What's the weather like?")
```

Addition to `Conversation` only works against `dict` objects that contain `"role"` and `"content"` items. Providing a `"chunk": True` flag into the object will merge it against the last message â€“ appending `"content"` and replacing other values.

### Completing and streaming Conversations

The `complete` and `stream_complete` methods facilitate interaction with the LLM based on the accumulated messages and configuration. These methods execute calls to generate responses and return them in form of a message object, but do not alter the conversation's `messages` list, allowing you to validate or modify the output before deciding to add it to the history.

<CodeGroup>
```python complete
# Using `complete` to get a single response
response = conversation.complete()
print("LLM Response:", response)
```
``` python stream_complete
# Using `stream_complete` to get streamed responses
for chunk in conversation.stream_complete():
    print("Streamed Message:", chunk)
    # Manually adding to the conversation
    conversation += chunk
```
</CodeGroup>



<Tip> 
First chunk of `stream_complete` is not flagged. When using `stream_complete`, the first chunk returned by the stream is not flagged explicitly as a "chunk." This behavior is by design, to seamlessly add this initial chunk into the conversation history if it is appended using the `+` operator.
</Tip>

Instance-wide configuration parameters can be complemented or overriden on individual call's level, if a `data` dictionary is provided to the method:

```python
# Overriding configuration for a specific call
response = conversation.complete(data={'max_tokens': 200, 'temperature': 0.5})
```

## Text completions without a conversation state
These `complete` and `stream_complete` methods are designed for one-off text completions without the need to manage a conversation state. They return the model's response as a string. Each function accepts a `data` dictionary allowing call-specific configurations.

<CodeGroup>
```python complete
# Using `complete` for a single completion
text_response = complete("Explore the benefits of AI.", data={'temperature': 0.3})
print("Completion:", text_response)
```
```python stream_complete
# Using `stream_complete` for streamed text completions
for text_chunk in stream_complete("Explore the benefits of AI.", data={'temperature': 0.3}):
    print("Streamed Text:", text_chunk)
```
</CodeGroup>


